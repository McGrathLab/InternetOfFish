{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjNw_Qku4FPC"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/khanhlvg/tflite_raspberry_pi/blob/main/object_detection/Train_custom_model_tutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gf2if_fGDaWc"
      },
      "source": [
        "##### Copyright 2021 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "35BJmtVpAP_n",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c634c1e4-2aa1-4600-fe11-86459b074bb3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 42.5 MB 1.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 213 kB 24.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.0 MB 42.3 MB/s \n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "tensorflow 2.8.0 requires tf-estimator-nightly==2.8.0.dev2021122109, which is not installed.\u001b[0m\n",
            "\u001b[K     |████████████████████████████████| 642 kB 12.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 10.9 MB 39.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 87 kB 6.6 MB/s \n",
            "\u001b[K     |████████████████████████████████| 77 kB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 237 kB 35.8 MB/s \n",
            "\u001b[K     |████████████████████████████████| 6.4 MB 33.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 3.4 MB 20.7 MB/s \n",
            "\u001b[K     |████████████████████████████████| 840 kB 41.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 596 kB 41.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 120 kB 44.0 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.2 MB 39.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 35.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 462 kB 51.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 25.3 MB 53.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 352 kB 51.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 99 kB 1.4 MB/s \n",
            "\u001b[K     |████████████████████████████████| 47.8 MB 1.7 MB/s \n",
            "\u001b[?25h  Building wheel for fire (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for py-cpuinfo (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "#!pip install -q tflite-model-maker\n",
        "!pip install -q tflite-support\n",
        "!pip install -q --use-deprecated=legacy-resolver tflite-model-maker\n",
        "!sudo apt-get install libportaudio2\n",
        "!pip install sounddevice"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4QQTXHHATDS"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import os\n",
        "\n",
        "from tflite_model_maker.config import ExportFormat, QuantizationConfig\n",
        "from tflite_model_maker import model_spec\n",
        "from tflite_model_maker import object_detector\n",
        "\n",
        "from tflite_support import metadata\n",
        "\n",
        "import tensorflow as tf\n",
        "assert tf.__version__.startswith('2')\n",
        "\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "from absl import logging\n",
        "logging.set_verbosity(logging.ERROR)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here we can download the train and validate dataset from the dropbox and upload it to google drive and the next cell would connect it"
      ],
      "metadata": {
        "id": "IYAxe8d6jAIm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "eYWUiNXnrcp8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2f47e101-3177-432e-ab0f-05f362d02703"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zPmvfG9_i_XG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yxh3KInCFeB-"
      },
      "source": [
        "## Train the object detection model\n",
        "\n",
        "### Step 1: Load the dataset\n",
        "\n",
        "* Images in `train_data` is used to train the custom object detection model.\n",
        "* Images in `val_data` is used to check if the model can generalize well to new images that it hasn't seen before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WiAahdsQAdT7"
      },
      "outputs": [],
      "source": [
        "train_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/drive/MyDrive/mbuna/Train',\n",
        "    '/content/drive/MyDrive/mbuna/Train',\n",
        "    ['Fish', 'Pipe']\n",
        ")\n",
        "\n",
        "val_data = object_detector.DataLoader.from_pascal_voc(\n",
        "    '/content/drive/MyDrive/mbuna/Validate',\n",
        "    '/content/drive/MyDrive/mbuna/Validate',\n",
        "    ['Fish', 'Pipe']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UNRhB8N7GHXj"
      },
      "source": [
        "### Step 2: Select a model architecture\n",
        "\n",
        "EfficientDet-Lite[0-4] are a family of mobile/IoT-friendly object detection models derived from the [EfficientDet](https://arxiv.org/abs/1911.09070) architecture.\n",
        "\n",
        "Here is the performance of each EfficientDet-Lite models compared to each others.\n",
        "\n",
        "| Model architecture | Size(MB)* | Latency(ms)** | Average Precision*** |\n",
        "|--------------------|-----------|---------------|----------------------|\n",
        "| EfficientDet-Lite0 | 4.4       | 146           | 25.69%               |\n",
        "| EfficientDet-Lite1 | 5.8       | 259           | 30.55%               |\n",
        "| EfficientDet-Lite2 | 7.2       | 396           | 33.97%               |\n",
        "| EfficientDet-Lite3 | 11.4      | 716           | 37.70%               |\n",
        "| EfficientDet-Lite4 | 19.9      | 1886          | 41.96%               |\n",
        "\n",
        "<i> * Size of the integer quantized models. <br/>\n",
        "** Latency measured on Raspberry Pi 4 using 4 threads on CPU. <br/>\n",
        "*** Average Precision is the mAP (mean Average Precision) on the COCO 2017 validation dataset.\n",
        "</i>\n",
        "\n",
        "In this notebook, we use EfficientDet-Lite0 to train our model. You can choose other model architectures depending on whether speed or accuracy is more important to you."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZOojrDHAY1J"
      },
      "outputs": [],
      "source": [
        "spec = model_spec.get('efficientdet_lite0')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5aeDU4mIM4ft"
      },
      "source": [
        "### Step 3: Train the TensorFlow model with the training data.\n",
        "\n",
        "* Set `epochs = 100`, which means it will go through the training dataset 100 times. You can look at the validation accuracy during training and stop when you see validation loss (`val_loss`) stop decreasing to avoid overfitting.\n",
        "* Set `batch_size = 8 or 16` here so you will see that it takes 239 steps to go through the 1912 images in the training dataset.\n",
        "* Set `train_whole_model=True` to fine-tune the whole model instead of just training the head layer to improve accuracy. The trade-off is that it may take longer to train the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_MClfpsJAfda",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7aa0e7a-2d1e-4f33-93bf-b28b530f19ca"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/75\n",
            "239/239 [==============================] - 195s 618ms/step - det_loss: 0.7385 - cls_loss: 0.4331 - box_loss: 0.0061 - reg_l2_loss: 0.0631 - loss: 0.8016 - learning_rate: 0.0090 - gradient_norm: 1.8565 - val_det_loss: 0.4064 - val_cls_loss: 0.2650 - val_box_loss: 0.0028 - val_reg_l2_loss: 0.0631 - val_loss: 0.4695\n",
            "Epoch 2/75\n",
            "239/239 [==============================] - 142s 596ms/step - det_loss: 0.3275 - cls_loss: 0.1935 - box_loss: 0.0027 - reg_l2_loss: 0.0632 - loss: 0.3906 - learning_rate: 0.0100 - gradient_norm: 2.0392 - val_det_loss: 0.2938 - val_cls_loss: 0.1625 - val_box_loss: 0.0026 - val_reg_l2_loss: 0.0632 - val_loss: 0.3570\n",
            "Epoch 3/75\n",
            "239/239 [==============================] - 142s 595ms/step - det_loss: 0.2822 - cls_loss: 0.1694 - box_loss: 0.0023 - reg_l2_loss: 0.0633 - loss: 0.3455 - learning_rate: 0.0100 - gradient_norm: 1.8868 - val_det_loss: 0.2507 - val_cls_loss: 0.1634 - val_box_loss: 0.0017 - val_reg_l2_loss: 0.0633 - val_loss: 0.3140\n",
            "Epoch 4/75\n",
            "239/239 [==============================] - 141s 592ms/step - det_loss: 0.2578 - cls_loss: 0.1567 - box_loss: 0.0020 - reg_l2_loss: 0.0633 - loss: 0.3211 - learning_rate: 0.0099 - gradient_norm: 1.7711 - val_det_loss: 0.2466 - val_cls_loss: 0.1723 - val_box_loss: 0.0015 - val_reg_l2_loss: 0.0633 - val_loss: 0.3100\n",
            "Epoch 5/75\n",
            "239/239 [==============================] - 178s 748ms/step - det_loss: 0.2385 - cls_loss: 0.1460 - box_loss: 0.0018 - reg_l2_loss: 0.0633 - loss: 0.3019 - learning_rate: 0.0099 - gradient_norm: 1.7025 - val_det_loss: 0.1812 - val_cls_loss: 0.1218 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0634 - val_loss: 0.2446\n",
            "Epoch 6/75\n",
            "239/239 [==============================] - 143s 600ms/step - det_loss: 0.2339 - cls_loss: 0.1429 - box_loss: 0.0018 - reg_l2_loss: 0.0634 - loss: 0.2973 - learning_rate: 0.0099 - gradient_norm: 1.6485 - val_det_loss: 0.2236 - val_cls_loss: 0.1494 - val_box_loss: 0.0015 - val_reg_l2_loss: 0.0634 - val_loss: 0.2869\n",
            "Epoch 7/75\n",
            "239/239 [==============================] - 139s 583ms/step - det_loss: 0.2266 - cls_loss: 0.1366 - box_loss: 0.0018 - reg_l2_loss: 0.0634 - loss: 0.2900 - learning_rate: 0.0098 - gradient_norm: 1.6521 - val_det_loss: 0.1874 - val_cls_loss: 0.1258 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0634 - val_loss: 0.2508\n",
            "Epoch 8/75\n",
            "239/239 [==============================] - 140s 585ms/step - det_loss: 0.2196 - cls_loss: 0.1338 - box_loss: 0.0017 - reg_l2_loss: 0.0634 - loss: 0.2830 - learning_rate: 0.0097 - gradient_norm: 1.6817 - val_det_loss: 0.1743 - val_cls_loss: 0.1136 - val_box_loss: 0.0012 - val_reg_l2_loss: 0.0634 - val_loss: 0.2377\n",
            "Epoch 9/75\n",
            "239/239 [==============================] - 140s 585ms/step - det_loss: 0.2124 - cls_loss: 0.1285 - box_loss: 0.0017 - reg_l2_loss: 0.0634 - loss: 0.2758 - learning_rate: 0.0097 - gradient_norm: 1.5395 - val_det_loss: 0.2029 - val_cls_loss: 0.1296 - val_box_loss: 0.0015 - val_reg_l2_loss: 0.0634 - val_loss: 0.2663\n",
            "Epoch 10/75\n",
            "239/239 [==============================] - 170s 711ms/step - det_loss: 0.1954 - cls_loss: 0.1199 - box_loss: 0.0015 - reg_l2_loss: 0.0634 - loss: 0.2588 - learning_rate: 0.0096 - gradient_norm: 1.4509 - val_det_loss: 0.1857 - val_cls_loss: 0.1404 - val_box_loss: 9.0499e-04 - val_reg_l2_loss: 0.0634 - val_loss: 0.2490\n",
            "Epoch 11/75\n",
            "239/239 [==============================] - 140s 587ms/step - det_loss: 0.2003 - cls_loss: 0.1239 - box_loss: 0.0015 - reg_l2_loss: 0.0634 - loss: 0.2637 - learning_rate: 0.0095 - gradient_norm: 1.5213 - val_det_loss: 0.1655 - val_cls_loss: 0.1213 - val_box_loss: 8.8288e-04 - val_reg_l2_loss: 0.0634 - val_loss: 0.2288\n",
            "Epoch 12/75\n",
            " 89/239 [==========>...................] - ETA: 1:08 - det_loss: 0.1945 - cls_loss: 0.1211 - box_loss: 0.0015 - reg_l2_loss: 0.0634 - loss: 0.2579 - learning_rate: 0.0094 - gradient_norm: 1.4307"
          ]
        }
      ],
      "source": [
        "model = object_detector.create(train_data, model_spec=spec, batch_size=8, train_whole_model=True, epochs=75, validation_data=val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KB4hKeerMmh4"
      },
      "source": [
        "### Step 4. Evaluate the model with the validation data.\n",
        "\n",
        "After training the object detection model using the images in the training dataset, use the 10 images in the validation dataset to evaluate how the model performs against new data it has never seen before.\n",
        "\n",
        "As the default batch size is 64, it will take 1 step to go through the 10 images in the validation dataset.\n",
        "\n",
        "The evaluation metrics are same as [COCO](https://cocodataset.org/#detection-eval)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OUqEpcYwAg8L",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "10f7f614-32af-4ad3-d007-a48e908bd99b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8/8 [==============================] - 32s 3s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.7081746,\n",
              " 'AP50': 0.91667926,\n",
              " 'AP75': 0.68138456,\n",
              " 'AP_/Fish': 0.4342431,\n",
              " 'AP_/Pipe': 0.98210603,\n",
              " 'APl': 0.76836395,\n",
              " 'APm': 0.33602268,\n",
              " 'APs': 0.0,\n",
              " 'ARl': 0.83150727,\n",
              " 'ARm': 0.5100467,\n",
              " 'ARmax1': 0.6288227,\n",
              " 'ARmax10': 0.7670299,\n",
              " 'ARmax100': 0.78565055,\n",
              " 'ARs': 0.0}"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "model.evaluate(val_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NARVYk9rGLIl"
      },
      "source": [
        "### Step 5: Export as a TensorFlow Lite model.\n",
        "\n",
        "Export the trained object detection model to the TensorFlow Lite format by specifying which folder you want to export the quantized model to. The default post-training quantization technique is [full integer quantization](https://www.tensorflow.org/lite/performance/post_training_integer_quant). This allows the TensorFlow Lite model to be smaller, run faster on Raspberry Pi CPU and also compatible with the Google Coral EdgeTPU."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_u3eFxoBAiqE"
      },
      "outputs": [],
      "source": [
        "model.export(export_dir='.', tflite_filename='Mbuna_11thMay_EffDet_0.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZcBmEigOCO3"
      },
      "source": [
        "### Step 6:  Evaluate the TensorFlow Lite model.\n",
        "\n",
        "Several factors can affect the model accuracy when exporting to TFLite:\n",
        "* [Quantization](https://www.tensorflow.org/lite/performance/model_optimization) helps shrinking the model size by 4 times at the expense of some accuracy drop.\n",
        "* The original TensorFlow model uses per-class [non-max supression (NMS)](https://www.coursera.org/lecture/convolutional-neural-networks/non-max-suppression-dvrjH) for post-processing, while the TFLite model uses global NMS that's much faster but less accurate.\n",
        "Keras outputs maximum 100 detections while tflite outputs maximum 25 detections.\n",
        "\n",
        "Therefore you'll have to evaluate the exported TFLite model and compare its accuracy with the original TensorFlow model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jbl8z9_wBPlr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "006ff58a-f8e7-4b76-87c8-6e402edd9f29"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "479/479 [==============================] - 1108s 2s/step\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'AP': 0.7008301,\n",
              " 'AP50': 0.9140255,\n",
              " 'AP75': 0.6698917,\n",
              " 'AP_/Fish': 0.42268288,\n",
              " 'AP_/Pipe': 0.97897726,\n",
              " 'APl': 0.7555523,\n",
              " 'APm': 0.3285496,\n",
              " 'APs': 0.033333335,\n",
              " 'ARl': 0.79698986,\n",
              " 'ARm': 0.46074766,\n",
              " 'ARmax1': 0.62886983,\n",
              " 'ARmax10': 0.7475595,\n",
              " 'ARmax100': 0.75404227,\n",
              " 'ARs': 0.1}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "model.evaluate_tflite('Mbuna_11thMay_EffDet_0.tflite', val_data)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v7zgUkdOUUnD"
      },
      "outputs": [],
      "source": [
        "# Download the TFLite model to your local computer.\n",
        "from google.colab import files\n",
        "files.download('Mbuna_11thMay_EffDet_0.tflite')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RWP3fEPaGNvd"
      },
      "source": [
        "## Compile the model for EdgeTPU\n",
        "\n",
        "Finally, we'll compile the model using `edgetpu_compiler` so that the model can run on [Google Coral EdgeTPU](https://coral.ai/).\n",
        "\n",
        "We start with installing the EdgeTPU compiler on Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kK6AN1xVAsCb"
      },
      "outputs": [],
      "source": [
        "!curl https://packages.cloud.google.com/apt/doc/apt-key.gpg | sudo apt-key add -\n",
        "!echo \"deb https://packages.cloud.google.com/apt coral-edgetpu-stable main\" | sudo tee /etc/apt/sources.list.d/coral-edgetpu.list\n",
        "!sudo apt-get update\n",
        "!sudo apt-get install edgetpu-compiler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIGSdzXkEzrj"
      },
      "source": [
        "**Note:** When training the model using a custom dataset, beware that if your dataset includes more than 20 classes, you'll probably have slower inference speeds compared to if you have fewer classes. This is due to an aspect of the EfficientDet architecture in which a certain layer cannot compile for the Edge TPU when it carries more than 20 classes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rzF6u0FZTAjF"
      },
      "source": [
        "Before compiling the `.tflite` file for the Edge TPU, it's important to consider whether your model will fit into the Edge TPU memory. \n",
        "\n",
        "The Edge TPU has approximately 8 MB of SRAM for [caching model paramaters](https://coral.ai/docs/edgetpu/compiler/#parameter-data-caching), so any model close to or over 8 MB will not fit onto the Edge TPU memory. That means the inference times are longer, because some model parameters must be fetched from the host system memory.\n",
        "\n",
        "One way to elimiate the extra latency is to use [model pipelining](https://coral.ai/docs/edgetpu/pipeline/), which splits the model into segments that can run on separate Edge TPUs in series. This can significantly reduce the latency for big models.\n",
        "\n",
        "The following table provides recommendations for the number of Edge TPUs to use with each EfficientDet-Lite model.\n",
        "\n",
        "| Model architecture | Minimum TPUs | Recommended TPUs\n",
        "|--------------------|-------|-------|\n",
        "| EfficientDet-Lite0 | 1     | 1     |\n",
        "| EfficientDet-Lite1 | 1     | 1     |\n",
        "| EfficientDet-Lite2 | 1     | 2     |\n",
        "| EfficientDet-Lite3 | 2     | 2     |\n",
        "| EfficientDet-Lite4 | 2     | 3     |\n",
        "\n",
        "If you need extra Edge TPUs for your model, then update `NUMBER_OF_TPUS` here:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JyptUjakAwzz"
      },
      "outputs": [],
      "source": [
        "NUMBER_OF_TPUS = 1\n",
        "\n",
        "!edgetpu_compiler Mbuna_11thMay_EffDet_0.tflite --num_segments=$NUMBER_OF_TPUS"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q tflite-support"
      ],
      "metadata": {
        "id": "cdd7tJ-YlaGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJYXucYWTGqZ"
      },
      "source": [
        "Finally, we'll copy the metadata, including the label file, from the original TensorFlow Lite model to the EdgeTPU model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8LY1WrgMJBFd"
      },
      "outputs": [],
      "source": [
        "from tflite_support import metadata\n",
        "\n",
        "populator_dst = metadata.MetadataPopulator.with_model_file('Mbuna_11thMay_EffDet_0_edgetpu.tflite')\n",
        "\n",
        "with open('Mbuna_11thMay_EffDet_0.tflite', 'rb') as f:\n",
        "  populator_dst.load_metadata_and_associated_files(f.read())\n",
        "\n",
        "populator_dst.populate()\n",
        "updated_model_buf = populator_dst.get_model_buffer()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VdRihInCJ3ie"
      },
      "outputs": [],
      "source": [
        "# Download the TFLite model compiled for EdgeTPU to your local computer.\n",
        "from google.colab import files\n",
        "files.download('Mbuna_11thMay_EffDet_0_edgetpu.tflite')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Mbuna_Object_Detection",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}